\chapter[LITERATURE REVIEW]{\huge LITERATURE REVIEW}
In this chapter are mentioned and described the most related works with this thesis for overt and imagined speech classification. At the beginning are described the earlier works for imagined speech. Then, some recent related works that used EEG data are briefly described. These descriptions include data acquisition, classification techniques used, databases, experimental design, and comparatives of these aspects between works. At the end of the chapter are mentioned the most recent overt speech works, which used ECoG data.\\

\section{Early Work for Imagined Speech}

Speech recognition has been a continuous increasing research field since the decade of the 70s. Then, the interest in speech recognition in inaudible environments increased for military purposes and later to find alternatives for people with speech problems. Due to that, some SSI technologies emerged \cite{denby2010silent}, such as articulator capture with EMA, characterization of the vocal tract using ultrasound, digital signals transform from a NAM microphone, analysis of glottal activity using electromagnetic sensors, and surface EMG of articulator's muscles or the larynx.\\

Later, with the advances of technology and the development of more sophisticated BCIs, the study of speech through mental activity started at the end of the 90s. Thus, the research described in \cite{suppes1997brain} was the first attempt to recognize words from EEG and MEG data. This work was carried with 100 trials from 7 imagined words produced by 7 subjects. Hence, the signals were recorded with two different technologies: \textbf{1.} 16 grid EEG sensors and \textbf{2.} a 64 channel Neuroscan cap.\\

Moreover, three different experimental conditions were performed, in which the average of some trials, used as training samples, was computed per channel to create prototypes that later would be compared with the test samples. In both sample types, the FFT was applied, followed by a Butterworth filtering and an IFFT. Hence, the classification step consisted of comparing the distances (MSE) between the processed test samples and each class prototype per channel. Then, the sample was associated with the class, with which the least MSE was obtained.\\

According to a hypothesis test, the results obtained in \cite{suppes1997brain} were significant but with many variabilities across the subject's data accuracies. Nevertheless, in this \linebreak[4]preliminary work was conclude that imagined speech recognition is feasible with simple experimental conditions.\\

Besides, an open question was made at the end of that work, which questioned the practicality of machine learning techniques dealing with Spatio-temporal data. Hence, this open question has been the motivation of further works.\\

Later, to develop an intuitive communication system, it was carried a research thesis in \cite{marek}, from which EEG signals were recorded with a 16 channel Electro-Cap. Besides, the samples came from several trials of 5 different words and sentences databases produced by 6 subjects. However, the number of trials used for imagined speech experiments were 5 per class.\\

Moreover, the processing step in \cite{marek} consisted of computing the STFT over signal's overlapped segments (some experiments related to the length's and shift's selection also were done). Also the first ($\Delta$) and second ($\Delta^2$) derivatives of the resulting coefficients, as well as $\Delta$'s average ($\bar{\Delta}$), were computed. Then, these features were concatenated across the channel's segments and reduced to 16 with LDA.\\

Next, for the classification step were built continuous HMMs (also experiments with several amounts of states and Gaussians were performed) per class, and the input samples were associated with the class from which a maximal likelihood was obtained. Besides, a LOO validation method was used to compute the classification scores and several experiment's types were carried.\\

Hence, subject-dependent (SD) and subject-independent (SI) experiments were performed. SD means that classifiers are trained and tested with samples of the same subject, while SI refers when classifiers are trained with samples of one or more subject's samples and tested with other subject's samples.\\

In the case of SI experiments, the accuracies were close to the chance. While for SD experiments, good results were achieved (with 60\% from the best average accuracy). For this reason, it was concluded in \cite{marek} that the data employed were SD.\\

Besides, the signals were captured in different sessions with the same subjects. Due to that, other SD experiments were carried that consisted of training the classifier with samples of one session and testing it with samples from another session. The results from these experiments showed that the data in this work were also session-dependent.\\

As final contributions from that work, some other experiments were carried to find the channel's contributions in the classification accuracies. Thus, these experiments consisted of leaving some channels out and performing the classification with the rest channels. The hypothesis was that the lower the accuracy, the more the channels contributed to the phenomenon.\\

$F7$ and $T5$ channels are localized in the Broca's and Wernicke's areas, respectively. Broca's area is responsible for the pronunciation fluency, while Wernicke's area for the semantic processing. Following the proposed hypothesis, accuracies obtained without the $F7$  and/or without $T5$ channel were compared with the accuracies provided by the rest channels.\\

Although worse results were obtained with $F7$ and $T5$ channels, the accuracies using just these channels did not significantly increase compared with the accuracy obtained with all channels. However, these experiments reflected that just the channels (7 more precisely) around the motor cortex, Broca's and Wernicke's areas were enough for imagined speech recognition.\\

Then, in \cite{porbadnigk2009eeg} were questioned the results obtained in \cite{marek}. The hypothesis to reject those results was that such performances might have been overestimated due to temporally correlated artifacts in the signals. This argument was stated since the words in \cite{marek} were presented in blocks.\\

Due to that, the words in \cite{porbadnigk2009eeg} were organized in the following different orders: blocks, blocks reordered, short blocks, randomized, and sequential.\\

Moreover, the processing, feature extraction, and classification steps were similar to those in \cite{marek}. However, features were computed with the DTCWT instead of using STFT. Hence, the signals were captured with a 128 channel Electro-cap, from which 16 channels were preselected due to the amplifier's limitations. Besides, the experiments were limited by using just the SD single session approach.\\

Thus,  the block mode (same as in \cite{marek}) provided the best average score with 45.5\% of accuracy, while others achieved the chance level. For this reason, in \cite{porbadnigk2009eeg} was concluded that temporally correlated artifacts indeed superimposed the signals when words were presented to subjects by blocks.\\

With this last work, it is noticed the difficulty and relevance involved in imagined speech data acquisition. Besides, it was suggested in \cite{porbadnigk2009eeg} some improvements for this research area, which included the use of words with semantic meaning, lengths normalization, recognition feedback to the subject, and more trials per class. However, this last aspect was not covered by the following works since their main objective was to explore the signal's characteristics with simple experiments.\\

The work mentioned in \cite{d2009toward} was the first in analyzing the signals by spectral sub-bands. The signals were acquired with a 128 Channel Sensor Net, from which 18 were rejected. Besides, the database was composed of 120 trials from 4 subjects, who imagined 2 syllabic classes in 3 different rhythms. Each rhythm represented the $\alpha$, $\beta$, or $\theta$ well-known EEG sub-bands (described in the next chapter). Hence, the 6 resulting combinations of classes and rhythms were defined as conditions, which the classifier had to recognize.\\

Furthermore, contrary to the other previous mentioned works, it was performed a preprocessing step in \cite{d2009toward} by an LPF to remove mean and linear trends. Also, it was used thresholds to suppress components of high energy since these signal's adequations would aid in improving the classifier's recognition. Then, the signal's processing consisted of extracting their envelopes with the HT, which was the base for the classification.\\

The classifier consisted of 6 filters associated with each condition, and which were obtained with the pseudoinverse of the envelope's averages of their corresponding training signals from the same channel. Next, each test signal's envelope was extracted and compared with all filters by their inner product, with which 6 measures were obtained. Then, each measure was summed across the channels, and the maximal measure was used to determine the most likely condition.\\

The resulting accuracies showed classification performances above the chance level for the three sub-bands, particularly in the $\beta$ sub-band with an average accuracy of 74.25\% across the subject's scores (these experiments were SI). However, further spectral analysis over the channel's distribution was done by subject, which was a useful tool to visualize variability between trial-by-trial samples. Due to that, it was concluded that averages across the subject's data were meaningful.\\

In the same year, it was carried in \cite{dasalla2009spatial} a work similar to motor imaginary works with BCIs. Due to that, SRPs were localized by grand averaging samples in time from channels associated with the motor cortex area (C3, Cz, and C4) to test if imagined speech resembles real speech movements.\\

The data collection consisted of samples produced by 3 subjects, who imagined 2 different vowels or did not produce any action. Hence, 50 trials from each of these 3 classes were captured with a 64 channel BioSemi ActiveTwo system and a sampling rate of 256 Hz. Although all channels were used for classification tasks, just three were employed to visualize the SRPs.\\

All samples were preprocessed with a BPF (1-45 Hz), and features were extracted with the CSP method. For this reason, classification experiments were performed with each pair of classes using an SVM (RBF). Besides, each experiment was SD and repeated 20 times since the training and test sets were composed of different samples on each iteration. This experimental approach provided statistics related to the classifier's capabilities, which was implemented by first time for imagined speech.\\

Hence, the average accuracies obtained across binary classifications and subjects ranged from 68\% to 78\%, which were above the chance. Furthermore, SRPs were localized with a negative trend followed by a positive shift (at approximately 300 milliseconds later) using the motor cortex channels for both vowel classes, while for no-action class, there was no apparent SRP, which was the expected behavior.\\

These results, supported by the symmetric patterns observed in the spatial feature's topographic images of such channels, showed that imagined speech mechanism was similar to real speech. Also, the authors envisioned their system as a natural and intuitive control method for EEG-based SSI. Due to that, recent researches focus on overt speech decoding with ECoG signals, which are briefly described in the last section of this chapter.

\section{Recent Works with EEG Data}
In general, the previous works described in the last section concluded the following:
\begin{itemize}
	\item Imagined speech recognition is feasible with simple experimental conditions \cite{suppes1997brain}.
	\item Channel's selection is possible without compromising the accuracy \cite{marek}.
	\item Classifiers might learn from artifacts rather than mental activity due to how the data was acquired \cite{porbadnigk2009eeg}.
	\item Better accuracies could be obtained with sub-bands decomposition. However, a more in-depth analysis is necessary to validate or reject that statement \cite{d2009toward}.
	\item Imagined speech can resemble movement-related potentials associated with real speech movements \cite{dasalla2009spatial}.
\end{itemize}

Furthermore, each of these works agreed about the high variability across the subject's and session's samples, making SI classification a challenging task. Due to that, most of the works described in this section carried SI experiments considering the methods used in the works previously mentioned.\\

Imagined speech classification was performed in \cite{torres2013analisis} with a database that consisted of 5 words in Spanish imagined by 27 subjects 12 times each. These signals were collected with a 14-channel Emotiv Epoc cap and an Fs of 128 Hz. The hypothesis in such work was that samples from words semantically related could provide higher scores than chance.\\

Due to that, different methods were used to test that hypothesis. Firstly, a CAR method was employed to enhance the SNR of the EEG signals. Then, a time-scale decomposition of each signal was obtained with the DWT using a Daubechies db2 mother wavelet, with which 5 detailed and one approximation coefficient sets were obtained (a description of wavelet decompositions is made in the next chapter).\\

Later, it was computed the RWE from each coefficient set (from each signal) except for the D1 coefficient's sets, which were rejected. These features were concatenated across the channels, with which a feature vector was built. The dimensions of that feature vectors varied due to the two different channel's set used: \textbf{1.} a set of 5 channels close to the Brocka's and Wernicke's areas, and \textbf{2.} the 14 channels.\\

Furthermore, SI classification experiments with both channel's set were performed. The classifiers used were: NB, SVM (linear), and RF. In the particular case of SVM, a one-versus-the-rest method was used to classify multiple classes. Hence, 10-fold cross-validation was carried for these experiments.\\

In the first set of experiments (using a subset of 5 channels), the best average accuracy obtained across the subject's experiments was 44.43\% with the RF classifier. Due to that, just the RF classifier was used in the second set of experiments, from which the contribution rate was also reported. The contribution rate consisted of dividing the 4-channel's accuracy by the 16-channel's accuracy. Besides, the best average accuracy obtained was 60.11\% by using all channels. This result outperformed the 5-channel's accuracy, with which 47.93\% was achieved.\\

Although the best accuracy was achieved with all channels (\textbf{2.}), the contribution rates showed that the 5 channel's considered in \textbf{1.} contributed in \textbf{2.} at least with a 50\% of the classification. Due to that, it was suggested in \cite{torres2013analisis} that in future works would be necessary to explore the channel's contributions to reduce the amount of data without compromising recognition performance. Besides, it was concluded that indeed words related semantically aided to obtain scores above the chance.\\

As a continuation of the work in \cite{torres2013analisis}, it was explored the channel's contribution in the work of \cite{torres2016implementing} with the same database and similar \textbf{a)} processing, \textbf{b)} feature extraction and \textbf{c)} classifiers. The differences or enhancements were the followings: \textbf{a)} a biorthogonal 2.2 wavelet mother used for DWT, \textbf{b)} four statistics features were computed additionally to RWE, \textbf{c)} just RF classifier was used.\\

Moreover, the channel's selection consisted of two stages: \textbf{1.} a Pareto's front, which was approached as a multi-objective (Wrapper) optimization problem dealing with the error rate (accuracy maximization) and the number of channels; \textbf{2.} single channel's set selection from the front, applying several FISs.\\

Due to these experiments, the best average accuracy obtained across the subjects was 68.18\%. Hence, the configuration that provided this accuracy consisted of a FIS with 3 membership functions for both input variables (error rate and the number of channels) and approximately 7 channels. These results provided statistical evidence that the proposed method improved the scores with 50\% fewer channels used in \cite{torres2013analisis}.\\

Later, a further work in \cite{salinas2017bag}, that used the same database as in \cite{torres2013analisis} and \cite{torres2016implementing}, implemented a bag of features, which consisted of generating codewords based on vector quantization. Besides, a Butterworth's LPF, signal's segmentation, histograms, and a multinomial NB classifier were used.\\

Furthermore, a genetic algorithm was employed to seek the configuration of 4 parameters that provided the best accuracies across SD classification experiments. These parameters were the feature's type (FFT or DWT Daubechies db4 coefficients), window size ($8\leq W \leq$128), window sliding ($8\leq M \leq$128), and the number of clusters ($K\leq 1000$). Due to that, the best parameter's set obtained consisted of FFT coefficients, $K=75$, $W=40$, and $M=8$.\\

Moreover, the classification experiments (SD) were composed of 3 approaches: \textbf{1.} considering each segment as independent, \textbf{2.} concatenating the segments by channel, \textbf{3.} considering spatial and temporal aspects with n-grams. Also, these approaches were tested with feature vectors and raw EEG signals. From all these experiments, the best average accuracy obtained was 68.93\%$\pm$12.43\% with raw EEG signals and using the classification approach \textbf{2.}\\

Finally, the methodology followed in \cite{salinas2017bag} was implemented with two other imagined speech databases. Nevertheless, the accuracies were lower than the previous related work and lower than chance in the case of using that in \cite{coretto2017open} and \cite{zhao2015classifying}, respectively. Due to that, it was concluded that methodologies could not be generalized and a more in-depth analysis must be carried by database used.\\

Other contemporary work in \cite{zhao2015classifying} described an ambitious database composed of samples coming from 8 subjects (4 were discarded), who imagined and pronounced (overt speech) 12 times (trials) 7 phonemic/syllabic and 4 words. These signals were obtained with a 64-channel Neuroscan Quick-cap and an Fs of 1000 Hz. Also, the purpose of that work was to create a database to understand multimodal relationships. Due to that, acoustic signals and facial features (from images) were also captured.\\

The acquisition of each sample in that work consisted of 4 phases: \textbf{1.} 5 seconds of rest, \textbf{2.} a stimulus state (visual and auditory) followed by 2 seconds of articulator's position, \textbf{3.} 5 seconds for imagined speech and \textbf{4.} a speaking state in which overt, acoustic and image signals were recorded. Hence, the database contains all these data and their corresponding sample's intervals in the case of EEG signals.\\

In the preprocessing step were included the removal of ocular artifacts using blind source separation, a BPF ranging from 1 to 50 Hz, mean subtraction of each channel, and a small Laplacian filtering using the neighborhood of adjacent channels.\\

Next, each preprocessed channel signal was divided into segments of 10\% of the total length, from which each pair had an overlap of 50\% of their lengths. Then, 21 features, their $\Delta$'s, and $\Delta^2$'s were extracted from the sample points of each segment. Besides, as each feature vector was composed of 1197 values, features were ranked by their Pearson correlations to reduce their dimensionalities. Thus the accuracies reported in \cite{zhao2015classifying} were obtained by using 5 of these features.\\

On the other hand, a channel's selection was based on the Pearson correlations between acoustic and imagined speech feature vectors (composed of 1197 features) from each channel. Then, the top 10 highest absolute correlations were selected, which most of them corresponded to the area's responsible for speech planning.\\

Moreover, SI classification experiments were performed with a LOO validation and 5 different pairs of binary sets (the 11 classes previously mentioned were grouped into these sets phonologically). These binary set pairs were: \textbf{1.} vowel-only vs. consonants (C/V), \textbf{2.} presence of nasal ($\pm$)Nasal, \textbf{3.} presence of bilabial ($\pm$)Bilabial, \textbf{4.} presence of high-front vowel ($\pm/iy/$) and \textbf{5.} presence of high-back vowel ($\pm/uw/$).\\

Besides, these binary sets were used in six modalities: EEG-only, facial features only, audio (acoustic) only, EEG and facial features, EEG and audio features, and all modalities. Thus, the classifiers used for these binary sets and modalities were a DBN and an SVM with two different kernels (quadratic and RBF).\\

The results reported in \cite{zhao2015classifying} belonged to average accuracies across all modalities using C/V and $\pm/uw/$ binary sets per classifier and subject data. Despite that an ANOVA showed variability in the results across modalities with the SVM, these accuracies showed positive results for multimodal systems development with accuracies above 90\% (per subject) with a DBN.\\

Currently, it is known that three other works have shown results using that database. It was reported in \cite{zhao2013combining} the same experiments from \cite{zhao2015classifying} but with a more in-depth analysis of the linear feature relationships between each pair of modalities, in which skewness, variance, and the sum of the first derivate of the signal were present in the top ten feature rankings. These preliminary results envisioned future works to reconstruct acoustic features with EEG signals in a multimodal system.\\

Next, one of the additional databases used in the previously mentioned work of \cite{salinas2017bag} was that of \cite{zhao2015classifying}. However, just the 4 words were used for classification using methods proposed in \cite{salinas2017bag}. The accuracies varied across the subjects and, consequently, the average accuracy was below the chance. However, it was mentioned that these results were not comparable with those from \cite{zhao2015classifying} due to the multimodal approach. Besides, it was concluded that further analysis must be done.\\

Then, the same feature vectors allocated in this database were used in \cite{yazaed} to perform SI experiments with the 11 classes instead of using the binary categories from  \cite{zhao2015classifying}. These features were averaged across the 10 channels selected in \cite{zhao2015classifying} to feed each classifier used in that work. Besides, by that time the database consisted of 14 subject samples which were employed in that work.\\

Furthermore, the classification experiments were two, which in general consisted of the following objectives:
\begin{enumerate}
	\item \underline{Classification with HMMs}: Experiments using normalized and non-normalized features were performed to compare their training scores obtained with \linebreak[4]continuous HMMs (same training samples were used for test). Thus, the \linebreak[4]normalization was made by using different subset configurations, with which an accuracy's improvement was obtained with normalization.
	\item \underline{Feature selection}: Binary classification of $/m/$ and $/n/$ classes were done with six classifiers: DNN, MLP, LR, SVM (quadratic), SVM (RBF), and KNN. Besides, feature selection using the Wrapper method was carried, with which feature vectors with an optimal length of 100 values were obtained. Hence, different cross-validation experiments were performed, from which, on average, the best scores were obtained with 15-fold cross-validation. Finally, a feature selection was made jointly with HMMs, and again the best scores were obtained with normalized features.
\end{enumerate}

Finally, Table \ref{Table: Comparatives_Imagined} summarizes the most representative works mentioned in this chapter for imagined speech classification. The enumerations \textbf{1.} and \textbf{2.} represent each experiment also enumerated in the explanations. Hence, the score's row represents the best (average) test accuracies obtained in each work across the experiments they performed, and the spatial's row means that if the work used each EEG channel data independently or not.

\begin{landscape}
\begin{table}[h!]
	\centering
	\caption{Comparatives across imagined speech classification related works.}
	\scalebox{0.9}{
	\begin{tabular}{|*{8}{c|}}
		\hline
		\textbf{Subjects} & 7 & 6 & 21 & 4 & 3 & 21 & 8 \\\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Classes}}} & 7 words & 5-11 words & 5 words & 2 syllables & 2 vowels and & 5 & 7 phones/syllables \\
		&       &       &       &   3 rythms    &   no-action    &       & 4 words \\\hline
		\textbf{Trials} & 100 & 5 & 20 & 120 & 50 & 33 & 12 \\\hline
		\textbf{Language} & English & English & English & English & English & Spanish & English \\\hline
		\textbf{Channels} & \textbf{1.} 16, \textbf{2.} 64 & 16 & 128 & 128 & 64 & 14 & 62 \\\hline
		\textbf{Fs (Hz)} & \textbf{1.} 678, \textbf{2.} 500 & 300 & 300 & 1024 & 256 & 128 & 1000 \\\hline
		\textbf{Channel's} & No    & 7 & 16 & 110 & 3 (SRP) & \textbf{1.} 4, \textbf{2.} 14 & 10 (correlation) \\
		\textbf{selection}  &       &       &       &       &       &       & \\\hline
		\multicolumn{1}{|c|}{\multirow{3}{*}{\textbf{Preprocessing}}} & No    & No    & No    & LPF, mean & BPF   & CAR   & BPF \\
		&       &       &       & and linear &       &       & Laplacian filter \\
		&       &       &       & trend remotion &       &       & \\\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Processing}}} & Filtering & STFT  & DTCWT & HT, BPF & No    & DWT   & segmentation \\
		& Avg. prototypes &       &       & elliptic &       & Daubechies db2 &  \\\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Features}}} & No    & STFT coeff. & DTCWT & HT coeff. & CSP   & RWE   & 21 statistical \\
		&       & $\Delta$, $\Delta^{2}$, $\bar{\Delta}$ &   coeff    &       &       &       & $\Delta$, $\Delta^{2}$ \\\hline
		\textbf{Feature's} & No    & 16 (LDA) & LDA   & No    & No    & \textbf{1.} 5, \textbf{2.} 70 & 5 (correlation) \\
		\textbf{selection}  &       &       &       &       &       &       & \\\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Classifier(s)}}} & Distance & HMM   & HMM   & Inner product & SVM   & NB, RF & SVM, DBN \\
		&   MSE    &       &       &  &       & SVM   &  \\\hline
		\textbf{Classication} & Multiclass & Multiclass & Multiclass & Multiclass & Binary & Multiclass & Binary \\\hline
		\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Scores}}} & 34\%-97\% & 60\% & 45.5\% & 74.25\% & 68\%-78\% & \textbf{1.} 47.93\% & $>$90\% \\
		&       &       &       &  &       &  \textbf{2.} 60.11\%  &  \\\hline
		\textbf{Validation} & No    & LOO   & LOO   & No    & No    & 10-FCV & LOO \\\hline
		\textbf{Initializations} & No    & No    & No    & No    & 20 & No    & No \\\hline
		\textbf{Spatial} & Yes   & No    & No    & Yes   & No    & No    & No \\\hline
		\textbf{Experiments} & SI    & SD,SI & SD    & SD,SI & SD    & SI    & SI \\\hline
		\textbf{Reference} & \cite{suppes1997brain} & \cite{marek} & \cite{porbadnigk2009eeg} & \cite{d2009toward} & \cite{dasalla2009spatial} & \cite{torres2013analisis} & \cite{zhao2015classifying} \\\hline
	\end{tabular}%
	}
	\label{Table: Comparatives_Imagined}%
\end{table}%
\end{landscape}

\section{Overt and Imagined Databases}

\begin{table}[h!]
	\centering
	\caption{Comparatives across overt and imagined speech databases.}
	\scalebox{0.9}{
	\begin{tabular}{|*{4}{c|}}
		\hline
		\textbf{Database} & Kara One & Brainliner & SINCI \\\hline
		\textbf{Speech} & Overt and imagined & imagined & Overt and imagined \\\hline
		\textbf{Subjects} & 14 & 3 & 15 \\\hline
		\textbf{Classes} & 11 & 3 & 11 \\\hline
		\textbf{Class' types} & phones, syllables, words & vowels & vowel, words \\\hline
		\textbf{Trials (Avg)} & \textbf{1.} 12, \textbf{2.} 12 & 50 & \textbf{1.} 12, \textbf{2.} 45 \\\hline
		\textbf{Total Trials} & \textbf{1.} 1913, \textbf{2.} 1913 & 450 & \textbf{1.} 1973, \textbf{2.} 7341 \\\hline
		\textbf{Language} & English & English & Spanish \\\hline
		\textbf{EEG channels} & 62 & 64 & 6 \\\hline
		\textbf{Fs (Hz)} & 1000 & 256 & 1024 \\\hline
		\textbf{Device} & NeuroScan & BioSemi ActiveTwo & Ag-AgCl electrodes \\\hline
		\textbf{Site} & \cite{karaone} & \cite{brainliner} & \cite{sinci} \\\hline
		\textbf{Reference} & \cite{zhao2015classifying} & \cite{dasalla2009single} & \cite{coretto2017open} \\\hline
	\end{tabular}%
	}
	\label{Table: Imagined_Databases}%
\end{table}%

Although some works related to imagined speech were mentioned in the previous section, not all databases are available online. Due to that, Table \ref{Table: Imagined_Databases} shows information from the three free imagined (and overt) speech databases founded.  As can be seen, the information varies among the databases.\\

The values \textbf{1.} and \textbf{2.} in the trial's rows represent, respectively, the amount of overt and imagined samples that the databases have. For the Kara One database, these amounts are balanced between both mental activities samples. Besides, the row $Trials (Avg)$ represents the number of trials (on average) generated by each subject, and the Fs showed from the Brainliner database represents the downsampled data since the original Fs was 2048 Hz. Also, the site's and reference's rows represent, respectively, the download Web page and the main work that used such database.\\

It can be noticed that the SINCI database has more samples than the rest. Nonetheless, fewer channels were used for the EEG recordings in the SINCI database than the others. Besides, contrary to \cite{sinci}, professional caps were employed for the captures in \cite{karaone} and \cite{brainliner}. Moreover, the Brainliner database counts with the least number of samples, as well as the least amount of subjects, and it only counts with imagined speech registers.\\

For these reasons and due to previous works (mentioned in the last section) that employed those data, the Kara One database was selected to perform the experiments in this work. 

%\section{Trends with ECoG Encoders}